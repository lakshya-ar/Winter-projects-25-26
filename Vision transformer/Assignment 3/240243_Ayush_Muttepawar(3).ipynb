{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTMSeaTLwVp"
      },
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TjKxTBEz8i7"
      },
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnVa1_8AKqtv"
      },
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERhYz9qV_YXI"
      },
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF5k5nKVL2OG"
      },
      "source": [
        "The gradient of a scalar function f with respect to an n × m matrix X has dimension n × m. \n",
        "\n",
        "This is because X contains nm independent scalar elements. The gradient is defined as the collection of partial derivatives of f with respect to each element xᵢⱼ of X. Since there is one partial derivative for each entry, the gradient has nm components and is naturally arranged in the same shape as X, resulting in an n × m matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnyYHlq2g4B"
      },
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrX2PmlcNDGZ"
      },
      "source": [
        "Accuracy- fraction of all correct predictions =(TP+TN)/total.\n",
        "\n",
        "Precision- ratio of correct positive predictions to the total predicted positives =TP/(TP+FP)\n",
        "\n",
        "Recall- ratio of correct positive predictions to the actual positive cases =TP/(TP+FN)\n",
        "\n",
        "F1 Score- It is the harmonic mean of Precision and Recall.\n",
        "\n",
        "From the table given,\n",
        "TP=80\n",
        "FP=80\n",
        "FN=20\n",
        "TN=820\n",
        "\n",
        "Accuracy= (80+820)/(80+80+20+820) = 0.9\n",
        "Precision= 80/160 = 0.5\n",
        "Recall= 80/(80+20) = 0.8\n",
        "F1 = 2(0.5*0.8)/(0.5+0.8) = 0.61\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1dJMD0vJKZg"
      },
      "source": [
        "3. What is a confusion matrix ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiQt25LLSnJh"
      },
      "source": [
        "Confusion matrix is a table that summarizes a classifier's predictions vs actual labels (counts of TP, FP, FN, TN).\n",
        "It contains values like true positives, false positives, false negatives and true negatives. It helps to understand where the model is performing well and where it is getting confused and is used to calculate metrics like accuracy, precision and recall etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      },
      "source": [
        "4. What is overfitting and underfitting ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aa29j-pTsZ0"
      },
      "source": [
        "Overfitting- when a model learns the training data too well including noises. Training accuracy is high but testing accuracy becomes low. This can be reduced using regularization, dropout, more data or early stopping.\n",
        "\n",
        "Underfitting- when a model is too simple to capture the pattern in data. Both training and testing accuracy are low. This can be solved by increasing model complexity or training for more time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4KfKc0z0ulm"
      },
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb4n01tyUGkQ"
      },
      "source": [
        "Vanishing gradients happen when gradients become very small during backpropagation, so learning slows down. Exploding gradients happen when gradients become too large and cause unstable updates. This usually occurs in deep networks due to repeated multiplication of weights. They can be reduced using proper weight initialization, ReLU activation, batch normalization, gradient clipping and smaller learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaOipvU1JoD"
      },
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHXvWg_9UucD"
      },
      "source": [
        "Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "In L1 regularization, the sum of absolute values of weights is added, which can make some weights exactly zero.\n",
        "\n",
        "In L2 regularization, the sum of squared weights is added, which keeps the weights small but not zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      },
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwceiKatUyrH"
      },
      "source": [
        "Dropout is a technique in which some neurons are randomly dropped during training. It is usually applied to the hidden layers of a neural network. This prevents the network from depending on specific neurons and forces it to learn better features, which helps in reducing overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZFLyv4-A67"
      },
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ABG0DbXvzl"
      },
      "source": [
        "Hidden activations -> a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "\n",
        "Dropout mask -> m = [1, 0, 0, 1, 1] dropout prob p = 0.5.\n",
        "\n",
        "Using the common inverted dropout (scale kept units by 1/(1-p) = 2 during training) :\n",
        "\n",
        "-Multiply elementwise by mask, then scale kept units by 2.\n",
        "\n",
        "index1: 2.0 * 1 * 2 = 4.0\n",
        "\n",
        "index2: 5.0 * 0 = 0.0\n",
        "\n",
        "index3: 7.12 * 0 = 0.0\n",
        "\n",
        "index4: 4.5 * 1 * 2 = 9.0\n",
        "\n",
        "index5: 6.0 * 1 * 2 = 12.0\n",
        "\n",
        "Final activation vector: [4.0, 0.0, 0.0, 9.0, 12.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La3K0HmUGj8t"
      },
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcLJX_JlV-R-"
      },
      "source": [
        "Gradient Descent (GD / batch): uses whole dataset each update. Stable gradients, but very slow for large data.\n",
        "\n",
        "Stochastic GD (SGD): uses one example per update. Very cheap per step, noisy updates, can escape shallow local minima.\n",
        "\n",
        "Mini-batch GD: uses small batches (eg. 32 - 256). Best trade-off — faster than GD, less noisy than SGD. Common in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngV9WtV7JxI"
      },
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCi_sE9Wx_2"
      },
      "source": [
        "Optimizers are methods used in neural network training to update the weights of the model. They use the gradients calculated from backpropagation and decide how much and in which direction the weights should change. A good optimizer helps the model to converge faster and reach minimum loss efficiently.\n",
        "\n",
        "Momentum Optimizer\n",
        "-\n",
        "Momentum is an improvement over normal gradient descent. In simple gradient descent, the weights are updated only based on current gradient, so the movement can be slow and zig-zag in some direction.\n",
        "\n",
        "In Momentum, we keep track of past gradients in the form of velocity. The update depends on both current gradient and previous velocity.\n",
        "\n",
        "-It helps in faster convergence\n",
        "\n",
        "-Reduces oscillations\n",
        "\n",
        "-Moves faster in correct direction\n",
        "\n",
        "Because of momentum, the optimizer does not change direction suddenly and becomes more smooth while learning.\n",
        "\n",
        "RMSprop Optimizer\n",
        "-\n",
        "RMSprop stands for Root Mean Square Propagation. It solves the problem of very large or very small learning rates.\n",
        "\n",
        "In RMSprop, we maintain a running average of squared gradients and divide the gradient by square root of this average. This helps in controlling the update size.\n",
        "\n",
        "-Large gradients get smaller updates\n",
        "\n",
        "-Small gradients get larger updates\n",
        "\n",
        "-Learning becomes more stable\n",
        "\n",
        "RMSprop works very well for non-stationary problems and deep neural networks.\n",
        "\n",
        "Adam Optimizer\n",
        "-\n",
        "Adam stands for Adaptive Moment Estimation. It is the most commonly used optimizer in practice.\n",
        "\n",
        "Adam combines the benefits of Momentum and RMSprop.\n",
        "\n",
        "-It uses moving average of gradients (first moment) like momentum\n",
        "\n",
        "-It also uses moving average of squared gradients (second moment) like RMSprop\n",
        "\n",
        "-Bias correction is applied to avoid zero initialization problem\n",
        "\n",
        "Because of these features, Adam converges faster and works well even with noisy data and sparse gradients. It usually gives good results without much tuning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
