{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since **f** is a scalar function, it relies on each element of the input matrix **X** and has a dimensionality of **nxm**, as the gradient is made up of partial derivatives concerning each entry of the matrix, resulting in a dimensionality of **nxm**."
      ],
      "metadata": {
        "id": "x5EmDXf6tfQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy measures the overall correctness of the model and is defined as the fraction of correctly classified samples out of the total number of samples.\n",
        "Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "2.  Precision measures how many of the predicted positive cases are actually positive.\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "3. Recall, also known as sensitivity, measures how many actual positive cases are correctly identified by the model.\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "4. The F1 score is the harmonic mean of precision and recall and balances both metrics.\n",
        "F1 score = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "From the given confusion matrix, taking “Cancer” as the positive class:\n",
        "TP = 80, FP = 80, FN = 20, TN = 820, and total samples = 1000.\n",
        "\n",
        "Accuracy = (80 + 820) / 1000 = 0.90 or 90%.\n",
        "Precision = 80 / (80 + 80) = 0.50 or 50%.\n",
        "Recall = 80 / (80 + 20) = 0.80 or 80%.\n",
        "F1 score ≈ 0.615 or 61.5%.\n",
        "\n",
        "Although accuracy is high, the relatively low precision indicates a large number of false cancer predictions."
      ],
      "metadata": {
        "id": "QCEVF37MtgBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **confusion matrix** is a tabular representation used to evaluate the performance of a classification model by comparing the predicted labels with the actual labels. It summarizes how well a model performs by showing the number of correct and incorrect predictions for each class. In binary classification, it consists of four values: True Positives, False Positives, False Negatives, and True Negatives which apart from helping us with calculating accuracy also tells the type of errors made, helping derive metrics such as precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "qtuijn8ZuPZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a tabular tool for judging a classification model’s performance by aligning predicted labels with the true ones. It encapsulates how well a model does by listing the count of correct and wrong predictions for each class. In binary classification, it holds four figures: True Positives, False Positives, False Negatives, and True Negatives, which not only help compute accuracy but also reveal the error types, guiding metrics like precision, recall, and F1 score. 1. Overfitting - when the model memorizes the data too closely, including noise and irrelevant patterns, leading to poor results on new data. 2. Underfitting - when the model is too simple to grasp the data’s underlying patterns, yielding weak performance on both training and test data."
      ],
      "metadata": {
        "id": "Fz4FZJ7eupwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Vanishing***  ***Gradient*** -occur when gradients shrink as they are propagated backward through many layers. This happens because gradients are repeatedly multiplied by small weights or derivatives of activation functions such as sigmoid or tanh, whose derivatives are less than one. As a result, gradients approach zero, causing earlier layers to learn very slowly or stop learning altogether.\n",
        "2. ***Exploding gradients*** - occur when gradients grow exponentially as they propagate backward. This is caused by repeatedly multiplying large weights or derivatives greater than one, leading to extremely large gradient values. It can cause unstable training, large parameter updates, and numerical overflow.\n",
        "\n",
        "* Vanishing gradients can be reduced by using activation functions like ReLU, proper weight initialization methods such as Xavier or He initialization, and batch normalization.\n",
        "* Exploding gradients can be controlled using gradient clipping, careful weight initialization, and smaller learning rates."
      ],
      "metadata": {
        "id": "nswCgxv_wWwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** is a technique used to prevent overfitting by adding penalty terms to loss functions that prevent complex models and large parameter values. This helps to generalize the model better to unseen data. * **L1 regularization** adds the sum of the absolute weight values as penalties. It promotes sparsity by forcing certain weights to be exactly zero and effectively performs feature selection. * **L2 regularization** adds the sum of the squared weight values as a penalty. It prevents heavy weights from being zero, resulting in smoother and more stable models."
      ],
      "metadata": {
        "id": "n3FAy9KHxJff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Dropout layer** is a regularization technique used to prevent overfitting in neural networks. During training, it randomly deactivates a fraction of neurons, forcing the network to learn more robust features.\n",
        "\n",
        "Dropout is generally applied to *hidden* *layers*, especially fully connected layers.\n",
        "\n",
        "By preventing neurons from becoming overly dependent on each other, dropout reduces co-adaptation and improves the model’s ability to generalize to unseen data."
      ],
      "metadata": {
        "id": "mLpGMZERxM2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how dropout works with the given activations:\n",
        "\n",
        "We start with activations a = [2.0, 5.0, 7.12, 4.5, 6.0] and a dropout probability p = 0.5.\n",
        "\n",
        "During training, the scaling factor (inverted dropout) is calculated as 1 / (1 - p) = 1 / 0.5 = 2.\n",
        "\n",
        "A dropout mask m = [1, 0, 0, 1, 1] is then applied.\n",
        "\n",
        "**Step 1: Applying the mask (dropping neurons)**\n",
        "\n",
        "We perform element-wise multiplication: [2.0, 5.0, 7.12, 4.5, 6.0] * [1, 0, 0, 1, 1] = [2.0, 0, 0, 4.5, 6.0].\n",
        "\n",
        "**Step 2: Scaling the kept activations**\n",
        "\n",
        "Multiply the remaining values by 2: [2.0 * 2, 0, 0, 4.5 * 2, 6.0 * 2].\n",
        "\n",
        "The final output activation vector is **[4.0, 0, 0, 9.0, 12.0]**."
      ],
      "metadata": {
        "id": "ghLUgGR1xew6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "| Aspect               | Gradient Descent (Batch GD)      | Stochastic Gradient Descent (SGD)         | Mini-Batch Gradient Descent                  |\n",
        "| -------------------- | -------------------------------- | ----------------------------------------- | -------------------------------------------- |\n",
        "| Data used per update | Uses the entire training dataset | Uses only one training example            | Uses a small subset (batch) of training data |\n",
        "| Update frequency     | One update per epoch             | One update per training sample            | Multiple updates per epoch                   |\n",
        "| Computation cost     | Very high for large datasets     | Very low per update                       | Moderate                                     |\n",
        "| Convergence behavior | Smooth and stable                | Noisy and fluctuating                     | Relatively smooth with some noise            |\n",
        "| Speed                | Slow for large datasets          | Fast but unstable                         | Faster and more stable                       |\n",
        "| Memory requirement   | High                             | Very low                                  | Moderate                                     |\n",
        "| Risk of local minima | Higher chance of getting stuck   | Can escape local minima due to noise      | Balanced behavior                            |               |\n",
        "| Example batch size   | Entire dataset                   | 1                                         | 16, 32, 64, etc.                             |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PUWZHqDlzAcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers** are algorithms used to update the weights of a neural network during training in order to minimize the loss function. They determine how the gradients are used to adjust model parameters and directly affect training speed and stability.\n",
        "\n",
        "***Momentum*** improves gradient descent by adding a fraction of the previous update to the current one. This helps accelerate learning in consistent directions and reduces oscillations.\n",
        "\n",
        "***RMSProp*** adapts the learning rate for each parameter by maintaining a moving average of squared gradients. It prevents large updates and stabilizes training, especially when gradients vary widely.\n",
        "\n",
        "***Adam*** combines the benefits of Momentum and RMSProp by keeping track of both the moving average of gradients and squared gradients. It is widely used due to its fast convergence and robustness."
      ],
      "metadata": {
        "id": "LaI_VR7Czcjt"
      }
    }
  ]
}